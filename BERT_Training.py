import pickle
import torch
import csv
from sklearn.ensemble._hist_gradient_boosting import predictor
from torch.utils.data import DataLoader
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
from sklearn import preprocessing

# from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification
from tqdm.auto import tqdm
from transformers import AdamW


class BERT_Training:
    def __init__(self, baseModel, tokenizer, num_of_epochs=10, learning_rate=5e-5):
        self.num_of_epochs = num_of_epochs
        self.learning_rate = learning_rate
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer, do_lower_case=True)
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.model = AutoModelForSequenceClassification.from_pretrained(baseModel, num_labels=1)
        self.model.to(self.device)

    def train(self, model, optimizer, train_loader):
        model.train()
        epoch_loss = 0
        size = len(train_loader.dataset)

        for i, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].type(torch.LongTensor).to(self.device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

            optimizer.zero_grad()
            loss = outputs.loss
            epoch_loss += loss.item()
            loss.backward()
            optimizer.step()

        print('Training loss: {:.3f}'.format(epoch_loss / size))

    def test(self, model, test_loader):
        model.eval()
        size = len(test_loader.dataset)
        test_loss, accuracy = 0, 0

        with torch.no_grad():
            for batch in test_loader:
                X, y = batch['input_ids'].to(self.device), batch['labels'].type(torch.LongTensor).to(self.device)
                pred = model(X, labels=y)

                test_loss += pred.loss
                accuracy += (pred.logits.softmax(1).argmax(1) == y).type(torch.float).sum().item()

            test_loss /= size
            accuracy /= size

            print("Test loss: {:.3f}, accuracy: {:.3f}%".format(test_loss, accuracy * 100))

    def prepare_data(self, train_descriptions, train_label, test_descriptions, test_label):
        X_train = train_descriptions.values.tolist()
        X_test = test_descriptions.values.tolist()

        X_train = self.tokenizer(X_train, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        X_test = self.tokenizer(X_test, return_tensors="pt", padding="max_length", max_length=128, truncation=True)

        train_dataset = Dataset(X_train, train_label)
        test_dataset = Dataset(X_test, test_label)

        self.train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        self.test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)

    def train_model(self, save_path):
        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)

        for i in range(self.num_of_epochs):
            print("Epoch: #{}".format(i + 1))
            self.train(self.model, optimizer, self.train_loader)
            self.test(self.model, self.test_loader)

        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)


class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


# Define the list of 8 base metrics
base_metrics = ['attackVector', 'attackComplexity', 'privilegesRequired', 'userInteraction', 'scope',
                'confidentialityImpact', 'integrityImpact', 'availabilityImpact']

# Load the data
data = pd.read_csv('Data/csv/data_2016.csv')
print(data)

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Iterate over the base metrics
for metric in base_metrics:
    # Set up the data specific to the current metric
    X_train = train_data['description']  # Update with the appropriate feature column
    y_train = train_data[metric]  # Update with the appropriate label column
    X_test = test_data['description']  # Update with the appropriate feature column
    y_test = test_data[metric]  # Update with the appropriate label column

    # Create an instance of the BERT_Training class for the current metric
    bert_trainer = BERT_Training(baseModel='./model/bert_uncased_L-4_H-512_A-8', tokenizer='./model/bert_uncased_L-4_H-512_A-8')

    # Prepare the data for training
    bert_trainer.prepare_data(train_descriptions=X_train, train_label=y_train, test_descriptions=X_test, test_label=y_test)

    # Specify the save path for the current metric's model
    save_path = f'Saved_Models_Temp/{metric}/'  # Update with the appropriate directory structure

    # Train the model for the current metric
    bert_trainer.train_model(save_path)
