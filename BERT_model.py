
import pickle

import torch
import csv

from sklearn.ensemble._hist_gradient_boosting import predictor
from torch.utils.data import DataLoader
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
from sklearn import preprocessing

# from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification
from tqdm.auto import tqdm
from transformers import AdamW


def save_pickle(obj, file_name):
    with open("Saved_Models/Scope/" + file_name, 'wb') as file:
        pickle.dump(obj, file)


# def read_pickle(filename):
#     with open("Saved_Models/" + filename, 'rb') as file:
#         obj = pickle.load(file)
#     # filename = "2002-2022-vendor-year-count.pickle"
#     return obj


class Dataset(torch.utils.data.Dataset):
    """
    Class to store the data as PyTorch Dataset
    """

    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # an encoding can have keys such as input_ids and attention_mask
        # item is a dictionary which has the same keys as the encoding has
        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


print(Dataset.__doc__)


class Predictor:
    """
    Class to for holding predictor object
    
    """

    def __init__(self, tokenizer='./model/bert_uncased_L-4_H-512_A-8', baseModel='./model/bert_uncased_L-4_H-512_A-8',
                 num_of_epochs=10, learning_rate=5e-5, num_labels=1):
        self.num_of_epochs = num_of_epochs
        self.learning_rate = learning_rate
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer, do_lower_case=True)
        self.encoder = preprocessing.LabelEncoder()
        # self.device = torch.device('cpu')
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.model = AutoModelForSequenceClassification.from_pretrained(baseModel, num_labels=num_labels)
        self.model.to(self.device)

    def train(self, model, optimizer):
        """Method to train the model"""
        dataloader = self.train_loader
        model.train()

        epoch_loss = 0
        size = len(dataloader.dataset)

        for i, batch in enumerate(dataloader):
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].type(torch.LongTensor).to(self.device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

            optimizer.zero_grad()
            loss = outputs.loss
            # print(loss)
            epoch_loss += loss.item()
            loss.backward()
            optimizer.step()

        print('Training loss: {:.3f}'.format(epoch_loss / size))

    print(train.__doc__)

    def test(self, model):
        """Method to test the model's accuracy and loss on the validation set"""
        dataloader = self.test_loader
        model.eval()

        size = len(dataloader.dataset)
        test_loss, accuracy = 0, 0

        with torch.no_grad():
            for batch in dataloader:
                X, y = batch['input_ids'].to(self.device), batch['labels'].type(torch.LongTensor).to(self.device)
                pred = model(X, labels=y)

                test_loss += pred.loss
                accuracy += (pred.logits.softmax(1).argmax(1) == y).type(torch.float).sum().item()

            test_loss /= size
            accuracy /= size

            print("Test loss: {:.3f}, accuracy: {:.3f}%".format(test_loss, accuracy * 100))

    def prepare_data(self, train_descriptions, train_label, test_descriptions, test_label):

        # label processing
        y_train = self.encoder.fit_transform(train_label)
        y_test = self.encoder.fit_transform(test_label)
        # labels = torch.tensor(label)
        # X_train, X_test, y_train, y_test = train_test_split(descriptions, label, test_size=0.2, random_state=42)

        # converting to list
        X_train = train_descriptions.values.tolist()
        X_test = test_descriptions.values.tolist()

        # some saving
        # self.X_train = X_train
        # self.X_test = X_test
        # self.y_train = y_train
        # self.y_test = y_test

        # tokenize
        X_train = self.tokenizer(X_train, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        X_test = self.tokenizer(X_test, return_tensors="pt", padding="max_length", max_length=128, truncation=True)

        # dataloader
        self.train_loader = DataLoader(Dataset(X_train, y_train), batch_size=32, shuffle=True)
        self.test_loader = DataLoader(Dataset(X_test, y_test), batch_size=32, shuffle=True)

    def train_model(self,
                    path_new_model="model/temp_model/"):

        # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')


        # optimizer
        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)

        # train_test = Train_test()

        tqdm.pandas()

        for name, param in self.model.named_parameters():
            if 'classifier' not in name:  # classifier layer
                param.requires_grad = False
            else:
                param.requires_grad = True

        for i in tqdm(range(self.num_of_epochs // 2)):
            print("Epoch: #{}".format(i + 1))
            self.train(self.model, optimizer)
            self.test(self.model)

        print("After Unfreezing the layer......................")

        for name, param in self.model.named_parameters():
            param.requires_grad = True

        # optimizer
        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=0.01)

        for i in tqdm(range(self.num_of_epochs // 2, self.num_of_epochs)):
            print("Epoch: #{}".format(i + 1))
            self.train(self.model, optimizer)
            self.test(self.model)

        self.model.save_pretrained(path_new_model)
        self.tokenizer.save_pretrained(path_new_model)

        # saving the model not saving optimizer
        # self.model = model

    def predict(self, descriptions):
        # torch.cuda.empty_cache()
        # preprocess data
        # import gc
        # del self.train
        # del self.test
        # gc.collect()
        X_test = descriptions.values.tolist()
        lent = len(X_test)
        X_test = predictor.tokenizer(X_test, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        # test_loader = DataLoader(Dataset(X_test, y_test), batch_size=64, shuffle=True)

        test_ids = []
        test_attention_mask = []

        test_ids.append(X_test['input_ids'])
        test_attention_mask.append(X_test['attention_mask'])
        test_ids = torch.cat(test_ids, dim=0)
        test_attention_mask = torch.cat(test_attention_mask, dim=0)

        self.model.eval()
        self.model.to("cpu")
        with torch.no_grad():
            # predictions = predictor.model(**X_test)
            predictions = self.model(test_ids.to("cpu"), token_type_ids=None,
                                     attention_mask=test_attention_mask.to("cpu"))

        predictions_class = []
        for i in range(lent):
            predictions_class.append(predictions.logits.softmax(1)[i].argmax().item())

        return self.encoder.inverse_transform(predictions_class)


if __name__ == '__main__':

    data = pd.read_csv('Data/data_2016.csv').dropna()
    data_test = pd.read_csv('Data/data_2017.csv').dropna()

    # # allcols = ['description','attackVector','attackComplexity','privilegesRequired','userInteraction','scope',
    # #         'confidentialityImpact','integrityImpact','availabilityImpact','baseScore','baseSeverity','exploitabilityScore','impactScore']
    # allcols = ['description', 'attackVector', 'attackComplexity', 'privilegesRequired', 'userInteraction', 'scope',
    #            'confidentialityImpact', 'integrityImpact', 'availabilityImpact']
    # # metc_clmn = ['attackVector','attackComplexity','privilegesRequired','userInteraction','scope',
    # #         'confidentialityImpact','integrityImpact','availabilityImpact','baseScore','baseSeverity','exploitabilityScore','impactScore']
    #
    # metc_clmn = ['attackVector', 'attackComplexity', 'privilegesRequired', 'userInteraction', 'scope',
    #              'confidentialityImpact', 'integrityImpact', 'availabilityImpact']
    # data = data[allcols].dropna()

    # av_data = data[['description', 'attackVector']]
    # X_train, X_test, y_train, y_test = train_test_split(data['description'], data['scope'], test_size=0.2, random_state=42)
    label_name = 'availabilityImpact'
    X_train = data['description']
    X_test = data_test['description']
    y_train = data[label_name]
    y_test = data_test[label_name]
    unique_label = list(set(y_train))
    num_labels = len(unique_label)
    print("Number of Label:", num_labels)
    predictor = Predictor(baseModel="./model/bert_uncased_L-4_H-512_A-8" ,
                          tokenizer="./model/bert_uncased_L-4_H-512_A-8", num_labels=num_labels)

    predictor.prepare_data(train_descriptions=X_train,
                           train_label=y_train,
                           test_descriptions=X_test,
                           test_label=y_test)

    predictor.train_model(path_new_model="Saved_Models/Availability Impact/")

    result = predictor.predict(X_test)

    y = y_test

    #
    #
    # #----------------------------------------------------------------------------
    field = ['X-Test', 'Result', 'Y_test']

    f = open('FinalOutput/ai.csv', 'w')
    writer = csv.writer(f)

    writer.writerow(field)

    for (a, b, c) in zip(X_test, result, y_test):
        row = [a, b, c]
        writer.writerow(row)

    f.close()

    # save_pickle(X_test, "Scope_X_Test.pickle")
    # save_pickle(result, "Scope_result.pickle")
    # save_pickle(y_test, "Scope_Y_Test.pickle")
    #

# %% re indexing

# y_test.to_csv('av_reidx.csv',index=False)

# y_test = pd.read_csv('av_reidx.csv')
