import pickle

import torch
import csv

from sklearn.ensemble._hist_gradient_boosting import predictor
from torch.utils.data import DataLoader
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
from sklearn import preprocessing

# from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification
from tqdm.auto import tqdm
from transformers import AdamW


def save_pickle(obj, file_name):
    with open("Saved_Models/Scope/" + file_name, 'wb') as file:
        pickle.dump(obj, file)


# def read_pickle(filename):
#     with open("Saved_Models/" + filename, 'rb') as file:
#         obj = pickle.load(file)
#     # filename = "2002-2022-vendor-year-count.pickle"
#     return obj


class Dataset(torch.utils.data.Dataset):
    """
    Class to store the data as PyTorch Dataset
    """

    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # an encoding can have keys such as input_ids and attention_mask
        # item is a dictionary which has the same keys as the encoding has
        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


print(Dataset.__doc__)


class Trainer:

    def __init__(self, tokenizer='./model/bert_uncased_L-4_H-512_A-8', baseModel='./model/bert_uncased_L-4_H-512_A-8',
                 num_of_epochs=2, learning_rate=5e-5, num_labels=1):
        self.num_of_epochs = num_of_epochs
        self.learning_rate = learning_rate
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer, do_lower_case=True)
        self.encoder = preprocessing.LabelEncoder()
        # self.device = torch.device('cpu')
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.model = AutoModelForSequenceClassification.from_pretrained(baseModel, num_labels=num_labels)
        self.model.to(self.device)

    def train(self, model, optimizer):
        """Method to train the model"""
        dataloader = self.train_loader
        model.train()

        epoch_loss = 0
        size = len(dataloader.dataset)

        for i, batch in enumerate(dataloader):
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].type(torch.LongTensor).to(self.device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

            optimizer.zero_grad()
            loss = outputs.loss
            # print(loss)
            epoch_loss += loss.item()
            loss.backward()
            optimizer.step()

        print('Training loss: {:.3f}'.format(epoch_loss / size))

    print(train.__doc__)


    def prepare_data(self, train_descriptions, train_label):

        # label processing
        y_train = self.encoder.fit_transform(train_label)


        # converting to list
        X_train = train_descriptions.values.tolist()


        # tokenize
        X_train = self.tokenizer(X_train, return_tensors="pt", padding="max_length", max_length=128, truncation=True)

        # dataloader
        self.train_loader = DataLoader(Dataset(X_train, y_train), batch_size=32, shuffle=True)

        # Set test loader to None
        self.test_loader = None

    def train_model(self, path_new_model="model/temp_model/"):
        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)
        tqdm.pandas()

        for name, param in self.model.named_parameters():
            if 'classifier' not in name:  # classifier layer
                param.requires_grad = False
            else:
                param.requires_grad = True

        for i in tqdm(range(self.num_of_epochs // 2)):
            print("Epoch: #{}".format(i + 1))
            self.train(self.model, optimizer)

        print("After Unfreezing the layer......................")

        for name, param in self.model.named_parameters():
            param.requires_grad = True

        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=0.01)

        for i in tqdm(range(self.num_of_epochs // 2, self.num_of_epochs)):
            print("Epoch: #{}".format(i + 1))
            self.train(self.model, optimizer)

        self.model.save_pretrained(path_new_model)
        self.tokenizer.save_pretrained(path_new_model)

    def predict(self, descriptions):
        # torch.cuda.empty_cache()
        # preprocess data
        # import gc
        # del self.train
        # del self.test
        # gc.collect()
        X_test = descriptions.values.tolist()
        lent = len(X_test)
        X_test = predictor.tokenizer(X_test, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        # test_loader = DataLoader(Dataset(X_test, y_test), batch_size=64, shuffle=True)

        test_ids = []
        test_attention_mask = []

        test_ids.append(X_test['input_ids'])
        test_attention_mask.append(X_test['attention_mask'])
        test_ids = torch.cat(test_ids, dim=0)
        test_attention_mask = torch.cat(test_attention_mask, dim=0)

        self.model.eval()
        self.model.to("cpu")
        with torch.no_grad():
            # predictions = predictor.model(**X_test)
            predictions = self.model(test_ids.to("cpu"), token_type_ids=None,
                                     attention_mask=test_attention_mask.to("cpu"))

        predictions_class = []
        for i in range(lent):
            predictions_class.append(predictions.logits.softmax(1)[i].argmax().item())

        return self.encoder.inverse_transform(predictions_class)


if __name__ == '__main__':

    # Load and preprocess the training data
    data = pd.read_csv('Data/csv/data_2016.csv').dropna()

    print(data)
    label_names = ['attackVector', 'attackComplexity', 'privilegesRequired', 'userInteraction', 'scope',
                   'confidentialityImpact', 'integrityImpact', 'availabilityImpact']

    for label_name in label_names:

        print(label_name,".........................................................................................")
        X_train = data['Description']
        y_train = data[label_name]
        unique_label = list(set(y_train))
        num_labels = len(unique_label)

        predictor = Trainer(baseModel="./model/bert_uncased_L-4_H-512_A-8",
                            tokenizer="./model/bert_uncased_L-4_H-512_A-8", num_labels=num_labels)

        print("Initializing done for label:", label_name)
        predictor.prepare_data(train_descriptions=X_train, train_label=y_train)
        print("Data preparation done for label:", label_name)

        predictor.train_model(path_new_model="Saved_Models2/"+label_name+"/")

        print("Training for label:", label_name, "completed.")

    print("Goodbye world")
